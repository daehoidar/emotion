# 수치예측 & 이진분류

## *선형 회귀

모델에 가장 적합한 일차함수 y = Wx+b를 찾아야 합니다.
y의 값을 변하게 하는 x를 독립 변수라고 하고, x에 따라서 바뀌는 y를 종속 변수라 합니다.

* **단순 선형 회귀**의 기본 형태  
▶ y=Wx+b  

머신러닝에서는 W는 가중치, b는 편향이라고 칭합니다. W와 b없이는 y = x라는 항등함수 밖에 없기 때문에 W, b는 꼭 필요합니다.

## *손실 함수, 경사 하강법
### *단순 선형 회귀
앞서 언급한 W, b를 찾기 위해 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세워서 이 식의 값을 최소화하는 W, b를 찾아 낸다. 이 때의 실제값과 예측값에 대한 오차에 대한 식을 손실 함수라 합니다.

 비용 함수의 식은 단순하게 실제 값과 에측값에 대한 오차를 예측값과 실제값과의 오차를 줄이는 일에도 최적화된 식이어야 합니다.머신러닝, 딥러닝에 다양한 문제들이 있고, 각 문제들에 적합한 비용 함수들이 있는데 회귀 문제에서는 주로 평균 제곱 오차가 사용됩니다.

 실제 값 - 예측값을 한 다음 제곱을 하여 모두 더한다. 그리고 이 오차들의 제곱의 평균을 더합니다.
![image](https://user-images.githubusercontent.com/45358775/68264380-00610080-008c-11ea-94c8-786a0bde902d.png)  
식으로 나타내면 다음과 같습니다.

### *경사하강법
머신러닝, 딥러닝 손실함수를 최소화하는 W, b를 찾기 위한 작업을 수행합니다. 그리고 이때 사용되는 것이 **옵티마이저 알고리즘, 최적화 알고리즘**이라고 합니다. **경사 하강법**은 가장 기본적인 옵티마이저 알고리즘입니다.

 경사를 따라 내려가면서 랜덤값 W값을 정한 뒤에, 점차W의 값을 수정해나갑니다. b도 마찬가지로 해서 경사 하강법을 통해 W, b에 대해서 최적의 W, b를 찾아갑니다.

# 이진 분류

## *퍼셉트론
**퍼셉트론**(정확히는 ‘인공 뉴런’ 혹은 ‘단순 퍼셉트론’ )에 대해 살펴보자면 퍼셉트론은 다수의 신호를 입력받아 하나의 신호를 출력합니다. 여기서 신호는 ‘흐른다’ 또는 ‘안 흐른다’(1 이나 0)의 두 가지의 값을 가질 수 있습니다. 퍼셉트론의 동작 원리를 수식으로 나타내면  

![image](https://user-images.githubusercontent.com/45358775/68264741-3b176880-008d-11ea-883a-9c13ea97e211.png)  

이 됩니다. 여기서  x1과 x2는 여기서는 ‘입력신호’, y는 ‘출력 신호’, W1와 W2은 ‘가중치’를 뜻합니다. 입력신호에 각각 고유한 가중치가 곱해지고 이 총합이 정해진 한계를 넘어설 때만 1을 출력합니다. 그 한계를 ‘임계값’이라 하고 θ(theta, 세타)로 나타냅니다. 가중치는 각 신호에 직접적으로 영향을 주어 각 신호의 영향력을 결정하는 역할을 합니다. 그리고 θ를 -b로 치환하면  

![image](https://user-images.githubusercontent.com/45358775/68264758-4e2a3880-008d-11ea-9f11-d5dd6bd536b2.png)  

이 됩니다. 여기서 b를 ‘편향’이라 합니다. 이 식에서 입력 신호에 가중치를 곱한 값을 편향값과 합쳐서 0보다 크면 1을 출력하고 그렇지 않으면 0을 출력합니다.  참고로 가중치와 편향값은 무수히 많은 값(;경우의 수)가 나올 수 있습니다.  

## *로지스틱 회귀
**로지스틱 회귀**는 이항형 또는 다항형이 될 수 있습니다. 이향형 로지스틱 회귀는 종속 변수(y)의 결과가 2개의 카태고리[ex)성공, 실패]로 존재하는 것을 의미하고 다항형 로지스틱 회귀는 2개 이상의 카테고리(맑음, 흐림, 비)로 나타낼 수 있습니다.  
 로지스틱 회귀는 선형 회귀와 유사하지만 종속 변수와 독립 변수 사이의 관계에 있어서 차이가 존재합니다. 이향형인 데이터에 적용했을 경우에 종속 변수y의 결과가 두가지[0, 1]로 제한된다는 것, 그리고 조건부 확률의 분포가 정규분포 대신 이항분포를 따른다는 점이 선형회귀와 다릅니다.
 
 ![image](https://user-images.githubusercontent.com/45358775/68265005-212a5580-008e-11ea-83bf-055b12621b91.png)(이항형)  
 
 ## *Sigmoid Function  
 **시그모이드 함수**는 활성화 함수들 중 하나로 식은 이렇게 됩니다.  
 
 ![image](https://user-images.githubusercontent.com/45358775/68265116-72d2e000-008e-11ea-8479-99bf0f65d2eb.png)  
 
 시그모이드 함수는 비선형 함수입니다. 여기서 비선형 함수를 신경망에서 사용하는 이유는 선형 함수를 사용했을 때는 은닉층을 사용하는 이점이 없기 때문입니다. 예를 들어 y = ax라는 선형 함수가 있고 이를 3층으로 구성하면 y = a(a(a(x)))가 되고 y = a^3x가 된다. y = a^3x로 표현된다는 것은 구지 은닉층으로 표현할 필요가 없습니다. 즉, 선형 함수로 아무리 층을 만들어 봤자 은닉층이 없어지고 이렇게 층을 만드는 것은 신경망에서의 은닉층의 이점으로 버리는 것이기 때문에 신경망에서는 비선형 함수를 사용합니다.  
 다른 비선형 함수인 계단 함수와 달리 시그모이드 함수의 매끄러움은 가중치 값을 전달할 때 좀 더 부드럽게 양을 조절해서 전달할 수 있습니다.  
 
![image](https://user-images.githubusercontent.com/45358775/68265316-09070600-008f-11ea-8273-7396b6949d1e.png)  


 


 
 
 
