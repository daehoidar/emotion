# 수치예측 & 이진분류

## *선형 회귀

모델에 가장 적합한 일차함수 y = Wx+b를 찾아야 합니다.
y의 값을 변하게 하는 x를 독립 변수라고 하고, x에 따라서 바뀌는 y를 종속 변수라 합니다.

* **단순 선형 회귀**의 기본 형태  
▶ y=Wx+b  

머신러닝에서는 W는 가중치, b는 편향이라고 칭합니다. W와 b없이는 y = x라는 항등함수 밖에 없기 때문에 W, b는 꼭 필요합니다.

## *손실 함수, 경사 하강법
### *단순 선형 회귀
앞서 언급한 W, b를 찾기 위해 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세워서 이 식의 값을 최소화하는 W, b를 찾아 낸다. 이 때의 실제값과 예측값에 대한 오차에 대한 식을 손실 함수라 합니다.

 비용 함수의 식은 단순하게 실제 값과 에측값에 대한 오차를 예측값과 실제값과의 오차를 줄이는 일에도 최적화된 식이어야 합니다.머신러닝, 딥러닝에 다양한 문제들이 있고, 각 문제들에 적합한 비용 함수들이 있는데 회귀 문제에서는 주로 평균 제곱 오차가 사용됩니다.

 실제 값 - 예측값을 한 다음 제곱을 하여 모두 더한다. 그리고 이 오차들의 제곱의 평균을 더합니다.
![image](https://user-images.githubusercontent.com/45358775/68264380-00610080-008c-11ea-94c8-786a0bde902d.png)  
식으로 나타내면 다음과 같습니다.

### *경사하강법
머신러닝, 딥러닝 손실함수를 최소화하는 W, b를 찾기 위한 작업을 수행합니다. 그리고 이때 사용되는 것이 옵티마이저 알고리즘, 최적화 알고리즘이라고 합니다. 경사 하강법은 가장 기본적인 옵티마이저 알고리즘입니다.

 경사를 따라 내려가면서 랜덤값 W값을 정한 뒤에, 점차W의 값을 수정해나갑니다. b도 마찬가지로 해서 경사 하강법을 통해 W, b에 대해서 최적의 W, b를 찾아갑니다.
